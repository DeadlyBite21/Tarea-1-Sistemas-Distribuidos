#!/usr/bin/env python3
"""
Comparador de Resultados - Experimentos de Cache
Une y compara los resultados de LRU, LFU y FIFO
"""

import json
import sys
import os
from datetime import datetime

def load_results(filename):
    """Carga resultados desde archivo JSON"""
    try:
        # Buscar en directorio results primero
        results_path = f"results/{filename}"
        if os.path.exists(results_path):
            filepath = results_path
        else:
            filepath = filename
            
        with open(filepath, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ Archivo {filename} no encontrado en results/ ni en directorio actual")
        return None
    except json.JSONDecodeError:
        print(f"âŒ Error leyendo archivo {filename}")
        return None

def format_duration(minutes):
    """Formatea duraciÃ³n en formato legible"""
    hours = int(minutes // 60)
    mins = int(minutes % 60)
    if hours > 0:
        return f"{hours}h {mins}m"
    else:
        return f"{mins}m"

def compare_results():
    """Compara los resultados de los tres experimentos"""
    
    print("ğŸ”„ CARGANDO RESULTADOS DE EXPERIMENTOS...")
    
    # Cargar resultados
    lru_results = load_results("lru_results.json")
    lfu_results = load_results("lfu_results.json")
    fifo_results = load_results("fifo_results.json")
    
    results = {}
    if lru_results:
        results["LRU"] = lru_results
        print("âœ… Resultados LRU cargados")
    if lfu_results:
        results["LFU"] = lfu_results
        print("âœ… Resultados LFU cargados")
    if fifo_results:
        results["FIFO"] = fifo_results
        print("âœ… Resultados FIFO cargados")
    
    if not results:
        print("âŒ No se encontraron archivos de resultados")
        print("ğŸ“‹ AsegÃºrate de que existan:")
        print("   â€¢ lru_results.json")
        print("   â€¢ lfu_results.json")
        print("   â€¢ fifo_results.json")
        return
    
    print(f"\n{'='*100}")
    print(f"ğŸ† COMPARACIÃ“N FINAL - EXPERIMENTOS DE CACHE 10,000 PREGUNTAS")
    print(f"{'='*100}")
    
    # Tabla de comparaciÃ³n
    print(f"\nğŸ“Š RESUMEN COMPARATIVO:")
    print(f"{'PolÃ­tica':<8} {'Hit Rate':<10} {'Hits':<8} {'Misses':<8} {'Evictions':<10} {'Cache Size':<10} {'DuraciÃ³n':<12}")
    print(f"{'-'*80}")
    
    best_policy = None
    best_hit_rate = 0
    
    for policy, stats in results.items():
        hit_rate = stats.get('hit_rate', 0)
        hits = stats.get('cache_hits', 0)
        misses = stats.get('cache_misses', 0)
        evictions = stats.get('evictions', 0)
        cache_size = stats.get('current_size', 0)
        
        # Obtener duraciÃ³n desde metadata si existe
        duration = "N/A"
        if 'experiment_metadata' in stats:
            duration_min = stats['experiment_metadata'].get('duration_minutes', 0)
            duration = format_duration(duration_min)
        
        print(f"{policy:<8} {hit_rate:<9.2%} {hits:<8,} {misses:<8,} {evictions:<10,} {cache_size:<10} {duration:<12}")
        
        if hit_rate > best_hit_rate:
            best_hit_rate = hit_rate
            best_policy = policy
    
    # Ganador
    if best_policy:
        print(f"\nğŸ¥‡ POLÃTICA GANADORA: {best_policy}")
        winner_stats = results[best_policy]
        print(f"   â€¢ Hit Rate: {best_hit_rate:.2%}")
        print(f"   â€¢ Cache Hits: {winner_stats.get('cache_hits', 0):,}")
        print(f"   â€¢ Evictions: {winner_stats.get('evictions', 0):,}")
        
        if 'experiment_metadata' in winner_stats:
            metadata = winner_stats['experiment_metadata']
            print(f"   â€¢ Preguntas procesadas: {metadata.get('questions_processed', 0):,}")
            print(f"   â€¢ Tiempo total: {format_duration(metadata.get('duration_minutes', 0))}")
    
    # AnÃ¡lisis detallado
    print(f"\nğŸ“ˆ ANÃLISIS DETALLADO:")
    
    for policy, stats in results.items():
        print(f"\nğŸ” {policy}:")
        print(f"   ğŸ“Š MÃ©tricas de Rendimiento:")
        print(f"      â€¢ Total requests: {stats.get('total_requests', 0):,}")
        print(f"      â€¢ Hit rate: {stats.get('hit_rate', 0):.3%}")
        print(f"      â€¢ Avg response time: {stats.get('avg_response_time', 0):.4f}s")
        print(f"      â€¢ Memory usage: {stats.get('memory_usage', 0)}")
        
        if 'experiment_metadata' in stats:
            metadata = stats['experiment_metadata']
            print(f"   â±ï¸ InformaciÃ³n del Experimento:")
            print(f"      â€¢ Inicio: {metadata.get('start_time', 'N/A')}")
            print(f"      â€¢ Fin: {metadata.get('end_time', 'N/A')}")
            print(f"      â€¢ DuraciÃ³n: {format_duration(metadata.get('duration_minutes', 0))}")
            print(f"      â€¢ Cache size configurado: {metadata.get('cache_size', 0)}")
            print(f"      â€¢ TTL: {metadata.get('ttl', 0)}s")
    
    # Recomendaciones
    print(f"\nğŸ’¡ RECOMENDACIONES:")
    
    if len(results) >= 3:
        lru_hit = results.get("LRU", {}).get('hit_rate', 0)
        lfu_hit = results.get("LFU", {}).get('hit_rate', 0)
        fifo_hit = results.get("FIFO", {}).get('hit_rate', 0)
        
        if lfu_hit > lru_hit and lfu_hit > fifo_hit:
            print("   ğŸ¯ LFU es la mejor opciÃ³n para este dataset")
            print("   ğŸ“‹ RazÃ³n: Aprovecha mejor la frecuencia de acceso de preguntas populares")
        elif lru_hit > lfu_hit and lru_hit > fifo_hit:
            print("   ğŸ¯ LRU es la mejor opciÃ³n para este dataset")
            print("   ğŸ“‹ RazÃ³n: Buen balance entre recencia y frecuencia")
        elif fifo_hit > 0:
            print("   ğŸ¯ FIFO muestra rendimiento bÃ¡sico")
            print("   ğŸ“‹ RazÃ³n: Simple pero menos eficiente para patrones de acceso")
        else:
            print("   âš ï¸ Todas las polÃ­ticas muestran hit rates bajos")
            print("   ğŸ“‹ Considerar: Mayor tamaÃ±o de cache o ajuste de TTL")
    
    # Guardar reporte comparativo
    comparison_report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'best_policy': best_policy,
            'best_hit_rate': best_hit_rate,
            'total_experiments': len(results)
        },
        'detailed_results': results
    }
    
    try:
        os.makedirs("results", exist_ok=True)
        report_path = "results/comparison_report.json"
        with open(report_path, 'w') as f:
            json.dump(comparison_report, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Reporte completo guardado en '{report_path}'")
    except Exception as e:
        print(f"âŒ Error guardando reporte: {e}")
    
    print(f"\nğŸ‰ ANÃLISIS COMPLETADO")

def main():
    print("ğŸ” COMPARADOR DE RESULTADOS - EXPERIMENTOS DE CACHE")
    print("ğŸ“‹ Este script compara los resultados de LRU, LFU y FIFO")
    print("ğŸ“ Busca archivos: lru_results.json, lfu_results.json, fifo_results.json")
    
    compare_results()

if __name__ == "__main__":
    main()